#load LD clump results
load(paste0("LD.clump.result.GA_",i1,".rdata"))
LD.clump.result <- LD.result.list[[1]]
LD.clump.result <- LD.clump.result[LD.clump.result$eth.vec!="EUR",]
method_vec <- rep("P+T",nrow(LD.clump.result))
LD.clump.result$method_vec = method_vec
LD.clump.result.PT <- LD.clump.result
#Best EUR result
load(paste0("best_eur_snp_result_GA_",i1,".rdata"))
method <- c("eurcoef","tarcoef","eb")
method_nameupdate <- c("Best EUR PRS","Best EUR SNP + target coefficients","Best EUR SNP + EB")
for(q in 1:length(method)){
idx <- which(LD.clump.result$method_vec==method[q])
LD.clump.result$method_vec[idx] <-   method_nameupdate[q]
}
LD.clump.result.EUR <- LD.clump.result
#
#2DLD
load(paste0("LD.clump.result.two.dim_GA_",i1,".rdata"))
LD.clump.result <- LD.result.list[[1]]
method_vec <- rep("2DLD",nrow(LD.clump.result))
LD.clump.result$method_vec = method_vec
LD.clump.result.2DLD = LD.clump.result
#2DLD + EB
# load(paste0("LD.clump.result.eb_GA_",i1,".rdata"))
# LD.clump.result <- LD.result.list[[1]]
# method_vec <- rep("2DLD + EB coefficients",nrow(LD.clump.result))
# LD.clump.result$method_vec = method_vec
# LD.clump.result.EB = LD.clump.result
#2WLD
load(paste0("LD.clump.result.two_way_GA_",i1,".rdata"))
LD.clump.result <- LD.result.list[[1]]
method_vec <- rep("2WLD",nrow(LD.clump.result))
LD.clump.result$method_vec = method_vec
LD.clump.result.2WLD = LD.clump.result
LD.clump.result <- rbind(LD.clump.result.EUR,LD.clump.result.PT,
LD.clump.result.2DLD,LD.clump.result.2WLD)
LD.clump.result$method_vec <- factor(LD.clump.result$method_vec,
levels = c("P+T",
"Best EUR PRS",
"Best EUR SNP + target coefficients",
"Best EUR SNP + EB",
"2DLD",
"2WLD"))
#"2DLD","2DLD-EB"))
sample_size =  as.character(LD.clump.result$method_vec)
ssp = as.character(c("15000","45000","80000","100000"))
cau_vec <- as.character(LD.clump.result$l_vec)
csp <- c(0.01,0.001,0.0005)
for(l in 1:3){
idx <- which(LD.clump.result$l_vec==l)
cau_vec[idx] <- paste0("Causal SNPs Proportion = ",csp[l])
}
cau_vec = factor(cau_vec,levels = paste0("Causal SNPs Proportion = ",csp))
for(m in 1:4){
idx <- which(LD.clump.result$m_vec==m)
sample_size[idx] <- ssp[m]
}
sample_size = factor(sample_size,
levels = c("15000","45000","80000","100000"))
LD.clump.result <- cbind(LD.clump.result,cau_vec,sample_size)
p <- ggplot(LD.clump.result,aes(x= sample_size,y=r2.vec,group=method_vec))+
geom_bar(aes(fill=method_vec),
stat="identity",
position = position_dodge())+
#geom_point(aes(color=method_vec))+
theme_Publication()+
ylab("R2")+
xlab("Sample Size")+
labs(fill = "Method")+
facet_grid(vars(cau_vec),vars(eth.vec))+
scale_fill_nejm()+
theme(axis.text = element_text(size = rel(0.9)),
legend.text = element_text(size = rel(0.9)))+
ggtitle("Prediction performance comparasion (Sample Size for EUR = 100k)")
p
png(file = paste0("./method_compare_result_summary_GA_",i1,".png"),
width = 13, height = 8, res = 300,units = "in")
print(p)
dev.off()
load("R2.simulation.hapmap3.RData")
res$cau_vec = as.character(res$cau_vec)
idx <- which(res$cau_vec=="Causal SNPs Proportion = 5e-4")
res$cau_vec[idx] = "Causal SNPs Proportion = 5e-04"
res$sample_size = as.character(res$sample_size)
idx <- which(res$sample_size=="1e+05")
res$sample_size[idx] = "100000"
res = res %>%
mutate(m_vec = case_when(sample_size%in%"15000"~1,
sample_size%in%"45000"~2,
sample_size%in%"80000"~3,
sample_size%in%"100000"~4))
res$sample_size = factor(res$sample_size,
levels = c("15000","45000","80000","100000"))
LD.result.LDpred <- res %>%
rename(eth.vec = race,
r2.vec = R2,
method_vec = Method) %>%
filter(GA==i1) %>%
select(eth.vec,r2.vec,cau_vec,sample_size,method_vec,m_vec)
LD.clump.result = LD.clump.result %>%
select(colnames(LD.result.LDpred))
LD.clump.result.plot = rbind(LD.clump.result,LD.result.LDpred)
save(LD.clump.result.plot,file = paste0("LD.clump.pred.result_GA_",i1,".rdata"))
for(m in 1:4){
LD.clump.result.sub <- LD.clump.result.plot %>% filter(m_vec==m) %>%
select(eth.vec,r2.vec,cau_vec,sample_size,method_vec) %>%
mutate(method_vec = as.character(method_vec))
method_vec = as.character(LD.clump.result.sub$method_vec)
method_vec = factor(method_vec,
levels = c("P+T","LDpred2",
"Best EUR PRS",
"Best EUR SNP + target coefficients",
"Best EUR SNP + EB",
"EUR LDpred2",
"2DLD",
"2WLD",
"MEBayes"))
LD.clump.result.sub$method_vec = method_vec
p <- ggplot(LD.clump.result.sub,aes(x= sample_size,y=r2.vec,group=method_vec))+
geom_bar(aes(fill=method_vec),
stat="identity",
position = position_dodge())+
#geom_point(aes(color=method_vec))+
theme_Publication()+
ylab("R2")+
xlab("Sample Size")+
labs(fill = "Method")+
facet_grid(vars(cau_vec),vars(eth.vec))+
#scale_fill_nejm()+
scale_fill_manual(values = getPalette(colourCount)) +
theme(axis.text = element_text(size = rel(0.9)),
legend.text = element_text(size = rel(0.9)))+
ggtitle("Prediction performance comparasion (Sample Size for EUR = 100k)")
p
png(file = paste0("./method_compare_result_size_",m,"_summary_GA_",i1,".png"),
width = 13, height = 8, res = 300,units = "in")
print(p)
dev.off()
}
LD.clump.result.15k <- LD.clump.result %>% filter(m_vec==1) %>%
select(eth.vec,r2.vec,cau_vec,sample_size,method_vec) %>%
mutate(method_vec = as.character(method_vec))
method_vec = as.character(LD.clump.result.15k$method_vec)
method_vec = factor(method_vec,
levels = c("P+T",
"Best EUR PRS",
"Best EUR SNP + target coefficients",
"Best EUR SNP + EB",
"2DLD",
"2WLD"))
LD.clump.result.15k$method_vec = method_vec
load("R2.simulation.hapmap3.RData")
res$cau_vec = as.character(res$cau_vec)
idx <- which(res$cau_vec=="Causal SNPs Proportion = 5e-4")
res$cau_vec[idx] = "Causal SNPs Proportion = 5e-04"
LD.result.LDpred <- res %>%
rename(eth.vec = race,
r2.vec = R2,
method_vec = Method) %>%
filter(GA==i1) %>%
select(eth.vec,r2.vec,cau_vec,sample_size,method_vec)
LD.clump.result.15k <- rbind(LD.clump.result.15k,LD.result.LDpred)
method_vec = as.character(LD.clump.result.15k$method_vec)
method_vec = factor(method_vec,
levels = c("P+T","LDpred2",
"Best EUR PRS",
"Best EUR SNP + target coefficients",
"Best EUR SNP + EB",
"EUR LDpred2",
"2DLD",
"2WLD",
"MEBayes"))
LD.clump.result.15k$method_vec = method_vec
colourCount = 9
getPalette = colorRampPalette(brewer.pal(9, "Paired"))
p <- ggplot(LD.clump.result.15k,aes(x= sample_size,y=r2.vec,group=method_vec))+
geom_bar(aes(fill=method_vec),
stat="identity",
position = position_dodge())+
#geom_point(aes(color=method_vec))+
theme_Publication()+
ylab("R2")+
xlab("Sample Size")+
labs(fill = "Method")+
facet_grid(vars(cau_vec),vars(eth.vec))+
#scale_fill_nejm()+
scale_fill_manual(values = getPalette(colourCount)) +
theme(axis.text = element_text(size = rel(0.9)),
legend.text = element_text(size = rel(0.9)))+
ggtitle("Prediction performance comparasion (Sample Size for EUR = 100k)")
p
png(file = paste0("./all_method_compare_result_size_",1,"_summary_GA_",i1,".png"),
width = 13, height = 8, res = 300,units = "in")
print(p)
dev.off()
}
#library(RColorBrewer)
colourCount = 9
getPalette = colorRampPalette(brewer.pal(9, "Set1"))
#
for(i1 in 1:2){
#standard LD clumping results
load(paste0("LD.clump.result.GA_",i1,".rdata"))
LD.clump.result.1kg <- LD.result.list[[1]]
chips = rep("1kg",nrow(LD.clump.result.1kg))
LD.clump.result.1kg$chips = chips
load(paste0("LD.clump.result.GA_",i1,"_mega.rdata"))
LD.clump.result.mega <- LD.result.list[[1]]
chips = rep("mega",nrow(LD.clump.result.mega))
LD.clump.result.mega$chips = chips
load(paste0("LD.clump.result.GA_",i1,"_hm.rdata"))
LD.clump.result.hm <- LD.result.list[[1]]
chips = rep("hm",nrow(LD.clump.result.mega))
LD.clump.result.hm$chips = chips
LD.clump.result = rbind(LD.clump.result.1kg,LD.clump.result.mega,LD.clump.result.hm)
#save(LD.clump.result,file = "LD.clump.result_090420_P+T.rdata")
#load("LD.clump.result_090420_P+T.rdata")
sample_size =  as.character(LD.clump.result$method_vec)
ssp = as.character(c("15000","45000","80000","100000"))
cau_vec <- as.character(LD.clump.result$l_vec)
csp <- c(0.01,0.001,0.0005)
for(l in 1:3){
idx <- which(LD.clump.result$l_vec==l)
cau_vec[idx] <- paste0("Causal SNPs Proportion = ",csp[l])
}
cau_vec = factor(cau_vec,levels = paste0("Causal SNPs Proportion = ",csp))
for(m in 1:4){
idx <- which(LD.clump.result$m_vec==m)
sample_size[idx] <- ssp[m]
}
sample_size = factor(sample_size,
levels = c("15000","45000","80000","100000"))
LD.clump.result <- cbind(LD.clump.result,cau_vec,sample_size)
LD.clump.result$eth.vec <- factor(LD.clump.result$eth.vec,
#levels =c("EUR","AFR","AMR","EAS","SAS"))
levels =c("EUR","AFR","AMR","EAS","SAS"))
for(m in 1:4){
idx <- which(LD.clump.result$m_vec ==m)
LD.clump.result.sub = LD.clump.result[idx,]
p <- ggplot(LD.clump.result.sub,aes(x= sample_size,y=r2.vec,group=chips))+
geom_bar(aes(fill=chips),
stat="identity",
position = position_dodge())+
#geom_point(aes(color=method_vec))+
theme_Publication()+
ylab("R2")+
xlab("Sample Size")+
labs(fill = "Method")+
facet_grid(vars(cau_vec),vars(eth.vec))+
scale_fill_nejm()+
theme(axis.text = element_text(size = rel(0.9)),
legend.text = element_text(size = rel(0.9)))+
ggtitle("Prediction performance comparasion (P+T)")
p
png(file = paste0("./LD_clumping_compare_GA_",i1,"_size_",m,".png"),
width = 10, height = 8, res = 300,units = "in")
print(p)
dev.off()
}
}
5*3*22*4*10
5*3*22*4*10*2
5*3*4*10*2
120000/15000
239/8
120000/5000
239/24
4*3*4*3*2
i =1
l = 1
i_rep =1
j = 1
i1 = 1
6600/3200
6600/3300
6600/3
6600/2
6600/1000
6600/3000
6600/3
5*3*10*22*2
m
1*3*4*3*2
72*3
72*4
72*4*4
pthres <- c(5E-08,1E-07,5E-07,1E-06,5E-06,1E-05,5E-05,1E-04,1E-03,1E-02,1E-01,0.5)
length(pthres)
5*3*10*22*2*4*12*5*4
5*3*10*22*2
6600/22
pthres <- c(5E-08,1E-07,5E-07,1E-06,5E-06,1E-05,5E-05,1E-04,1E-03,1E-02,1E-01,0.5)
#test pcakge
library(rsample)
install.packages("rsample")
install.packages("recipes")
install.packages("recipes")
install.packages("h2o")
#test pcakge
library(rsample)
library(recipes)
library(h2o)
#test pcakge
library(rsample)
library(recipes)
library(h2o)
install.packages("recipes")
library(recipes)
installl.packages("generics")
install.packages("generics")
install.packages("generics")
#test pcakge
library(rsample)
library(recipes)
library(h2o)
ames <- AmesHousing::make_ames()
#test pcakge
library(rsample)
library(recipes)
library(h2o)
ames <- AmesHousing::make_ames()
install.packages("AmesHousing")
library(AmesHousing)
ames <- AmesHousing::make_ames()
set.seed(123)  # for reproducibility
split <- initial_split(ames, strata = "Sale_Price")
head(split)
head(ames)
class(ames)
blueprint <- recipe(Sale_Price ~ ., data = ames_train) %>%
step_other(all_nominal(), threshold = 0.005)
ames_train <- training(split)
ames_test <- testing(split)
blueprint <- recipe(Sale_Price ~ ., data = ames_train) %>%
step_other(all_nominal(), threshold = 0.005)
train_h2o <- prep(blueprint, training = ames_train, retain = TRUE) %>%
juice() %>%
as.h2o()
ames <- AmesHousing::make_ames()
set.seed(123)  # for reproducibility
split <- initial_split(ames, strata = "Sale_Price")
ames_train <- training(split)
ames_test <- testing(split)
# Make sure we have consistent categorical levels
blueprint <- recipe(Sale_Price ~ ., data = ames_train) %>%
step_other(all_nominal(), threshold = 0.005)
train_h2o <- prep(blueprint, training = ames_train, retain = TRUE) %>%
juice() %>%
as.h2o()
h2o.init()
#test pcakge
install.packages("SuperLearner")
install.packages(c("caret", "glmnet", "randomForest", "ggplot2", "RhpcBLASctl"))
install.packages("xgboost", repos=c("http://dmlc.ml/drat/", getOption("repos")), type="source")
data(Boston, package = "MASS")
colSums(is.na(Boston))
outcome = Boston$medv
data = subset(Boston, select = -medv)
str(data)
dim(data)
set.seed(1)
train_obs = sample(nrow(data), 150)
x_train = data[train_obs, ]
x_holdout = data[-train_obs, ]
outcome_bin = as.numeric(outcome > 22)
y_train = outcome_bin[train_obs]
y_holdout = outcome_bin[-train_obs]
table(y_train, useNA = "ifany")
library(SuperLearner)
listWrappers()
sl_lasso = SuperLearner(Y = y_train, X = x_train, family = binomial(),
SL.library = "SL.glmnet")
sl_lasso
sl_lasso$cvRisk[which.min(sl_lasso$cvRisk)]
sl_rf = SuperLearner(Y = y_train, X = x_train, family = binomial(),
SL.library = "SL.ranger")
y_train = outcome_bin[train_obs]
y_holdout = outcome_bin[-train_obs]
library(ranger)
install.packages("ranger")
library(ranger)
sl_rf = SuperLearner(Y = y_train, X = x_train, family = binomial(),
SL.library = "SL.ranger")
sl = SuperLearner(Y = y_train, X = x_train, family = binomial(),
SL.library = c("SL.mean", "SL.glmnet", "SL.ranger"))
pred = predict(sl, x_holdout, onlySL = TRUE)
set.seed(1)
system.time({
# This will take about 2x as long as the previous SuperLearner.
cv_sl = CV.SuperLearner(Y = y_train, X = x_train, family = binomial(),
# For a real analysis we would use V = 10.
V = 3,
SL.library = c("SL.mean", "SL.glmnet", "SL.ranger"))
})
summary(cv_sl)
data = subset(Boston, select = -medv)
str(data)
dim(data)
set.seed(1)
train_obs = sample(nrow(data), 150)
x_train = data[train_obs, ]
x_holdout = data[-train_obs, ]
outcome_bin = as.numeric(outcome > 22)
y_train = outcome_bin[train_obs]
y_holdout = outcome_bin[-train_obs]
table(y_train, useNA = "ifany")
library(SuperLearner)
listWrappers()
outcome_bin = outcome
y_train = outcome_bin[train_obs]
y_holdout = outcome_bin[-train_obs]
table(y_train, useNA = "ifany")
library(SuperLearner)
listWrappers()
?glm
sl_lasso = SuperLearner(Y = y_train, X = x_train, family = gaussian(),
SL.library = "SL.glmnet")
sl_lasso
sl_lasso$cvRisk[which.min(sl_lasso$cvRisk)]
sl_rf = SuperLearner(Y = y_train, X = x_train, family = gaussian(),
SL.library = "SL.ranger")
sl = SuperLearner(Y = y_train, X = x_train, family = gaussian(),
SL.library = c("SL.mean", "SL.glmnet", "SL.ranger"))
pred = predict(sl, x_holdout, onlySL = TRUE)
head(pred)
sl = SuperLearner(Y = y_train, X = x_train, family = gaussian(),
SL.library = c("SL.glmnet", "SL.ranger"))
pred = predict(sl, x_holdout, onlySL = TRUE)
head(pred)
summary(y_holdout~pred[,1])
model1 <- y_holdout~pred[,1]
summary(model1)
model1 <- lm(y_holdout~pred[,1])
summary(model1)
dim(pred)
pred
class(pred)
model1 <- lm(y_holdout~pred$pred)
summary(model1)
result.data <- data.frame(r2.vec.test,r2.vec.vad,
pthres_vec,r2_ind_vec,
wc_ind_vec)
#}
library(SuperLearner)
library(ranger)
#}
prs.sum = colSums(prs.mat)
#drop the prs with all 0
prs.mat <- prs.mat[,idx]
x.train = prs.mat[1:n.test,]
cv_sl = CV.SuperLearner(Y = y.test, X = as.data.frame(x.test), family = gaussian(),
# For a real analysis we would use V = 10.
V = 3,
SL.library = c( "SL.glmnet", "SL.ranger"))
5*3*10*2
#evaluate the best prs performance on the validation
model <- summary(y.vad~y.pred[[1]])
data(Boston, package = "MASS")
data(Boston, package = "MASS")
sl_lib = c("SL.xgboost", "SL.randomForest", "SL.glmnet", "SL.nnet", "SL.ksvm",
"SL.bartMachine", "SL.kernelKnn", "SL.rpartPrune", "SL.lm", "SL.mean")
Boston$medv
# Fit XGBoost, RF, Lasso, Neural Net, SVM, BART, K-nearest neighbors, Decision Tree,
# OLS, and simple mean; create automatic ensemble.
result = SuperLearner(Y = Boston$medv, X = Boston[, -14], SL.library = sl_lib)set.seed(1)
set.seed(1)
sl_lib = c("SL.xgboost", "SL.randomForest", "SL.glmnet", "SL.nnet", "SL.ksvm",
"SL.bartMachine", "SL.kernelKnn", "SL.rpartPrune", "SL.lm", "SL.mean")
# Fit XGBoost, RF, Lasso, Neural Net, SVM, BART, K-nearest neighbors, Decision Tree,
# OLS, and simple mean; create automatic ensemble.
result = SuperLearner(Y = Boston$medv, X = Boston[, -14], SL.library = sl_lib)
install.packages("bartMachine")
install.packages("kernlab")
install.packages("kernelab")
install.packages("kernlab")
library(kernlab)
library(bartMachine)
install.packages("rJava")
library(kernlab)
library(bartMachine)
library(rJava)
library(rJava)
install.packages("rJava")
library(rJava)
sl_lib = c("SL.xgboost", "SL.randomForest", "SL.glmnet", "SL.nnet", "SL.ksvm",
"SL.bartMachine", "SL.kernelKnn", "SL.rpartPrune", "SL.lm", "SL.mean")
# Fit XGBoost, RF, Lasso, Neural Net, SVM, BART, K-nearest neighbors, Decision Tree,
# OLS, and simple mean; create automatic ensemble.
result = SuperLearner(Y = Boston$medv, X = Boston[, -14], SL.library = sl_lib)
install.packages("KernelKnn")
#install.packages("rJava")
#install.packages("bartMachine")
#install.packages("KernelKnn")
#library(rJava)
library(kernlab)
library(bartMachine)
library(KernelKnn)
sl_lib = c("SL.xgboost", "SL.randomForest", "SL.glmnet", "SL.nnet", "SL.ksvm",
"SL.kernelKnn", "SL.rpartPrune", "SL.lm", "SL.mean")
# Fit XGBoost, RF, Lasso, Neural Net, SVM, BART, K-nearest neighbors, Decision Tree,
# OLS, and simple mean; create automatic ensemble.
result = SuperLearner(Y = Boston$medv, X = Boston[, -14], SL.library = sl_lib)
result
train_obs = sample(nrow(data), 150)
x_train = data[train_obs, ]
x_holdout = data[-train_obs, ]
outcome_bin = outcome
y_train = outcome_bin[train_obs]
y_holdout = outcome_bin[-train_obs]
sl_lasso = SuperLearner(Y = y_train, X = x_train, family = gaussian(),
SL.library = sl_lib)
